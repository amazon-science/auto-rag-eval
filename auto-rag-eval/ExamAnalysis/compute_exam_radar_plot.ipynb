{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import glob\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from item_response_models import ExamSetting\n",
    "\n",
    "@dataclass\n",
    "class ExamSetting:\n",
    "    llm: str\n",
    "    retrieval: str\n",
    "    icl: int\n",
    "    name: str\n",
    "    path_pattern: str  # Assuming base path is a constant attribute of the class\n",
    "\n",
    "    def find_file_path(self):\n",
    "        \"\"\"\n",
    "        Find the file path using the class attributes.\n",
    "        \"\"\"\n",
    "        # Search for files matching the pattern\n",
    "        matching_files = glob.glob(self.path_pattern)\n",
    "        \n",
    "        # Return the first matching file or None\n",
    "        if matching_files is None or matching_files == []:\n",
    "            raise ValueError(f\"Incorrect path pattern {self.path_pattern}\")\n",
    "\n",
    "        return matching_files[0]\n",
    "    \n",
    "    @property\n",
    "    def exists(self):\n",
    "\n",
    "        # Search for files matching the pattern\n",
    "        matching_files = glob.glob(self.path_pattern)\n",
    "\n",
    "        return matching_files is not None and matching_files != []\n",
    "\n",
    "    @property\n",
    "    def data_path(self):\n",
    "        \"\"\"\n",
    "        Property to get the data path.\n",
    "        \"\"\"\n",
    "        return self.find_file_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_students(model, task):\n",
    "\n",
    "    root_path = f'{Path(\".\").resolve().parent}/Data/{task}/EvalResults'\n",
    "\n",
    "    extended_students = [\n",
    "        [ExamSetting(path_pattern=f'{root_path}/{task}Exam/llamav2/13b/full_sample_{task}Exam_closed_book_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='closed_book',\n",
    "                     icl=i,\n",
    "                     name=f'ClosedBook@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/13b/full_sample_{task}Exam_rag_siamese_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='rag_siamese',\n",
    "                     icl=i,\n",
    "                     name=f'Rag Siamese@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/13b/full_sample_{task}Exam_rag_dpr_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='rag_dpr',\n",
    "                     icl=i,\n",
    "                     name=f'Rag DPR@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/13b/full_sample_{task}Exam_rag_bm25_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='rag_bm25',\n",
    "                     icl=i,\n",
    "                     name=f'Rag BM25@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/llamav2/13b/full_sample_{task}Exam_rag_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='rag_multi_qa',\n",
    "                     icl=i,\n",
    "                     name=f'Rag MultiQA@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/llamav2/13b/full_sample_{task}Exam_rag_dpr_bm25_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='rag_dprv2',\n",
    "                     icl=i,\n",
    "                     name=f'Rag DPRV2@{i} [13B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}Exam/llamav2/13b/full_sample_{task}Exam_open_book_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:13B',\n",
    "                     retrieval='open_book',\n",
    "                     icl=i,\n",
    "                     name=f'Oracle@{i} [13B]')] \n",
    "        for i in range(3)\n",
    "    ]\n",
    "\n",
    "    # Add 70B Models\n",
    "    extended_students.extend([[\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}Exam/llamav2/70b/full_sample_{task}Exam_closed_book_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='llamav2:70B',\n",
    "                    retrieval='closed_book',\n",
    "                    icl=i,\n",
    "                    name=f'ClosedBook@{i} [70B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/70b/full_sample_{task}Exam_rag_siamese_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='llamav2:70B',\n",
    "                    retrieval='rag_siamese',\n",
    "                    icl=i,\n",
    "                    name=f'Rag Siamese@{i} [70B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/70b/full_sample_{task}Exam_rag_dpr_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='llamav2:70B',\n",
    "                    retrieval='rag_dpr',\n",
    "                    icl=i,\n",
    "                    name=f'Rag DPR@{i} [70B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/llamav2/70b/full_sample_{task}Exam_rag_bm25_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='llamav2:70B',\n",
    "                    retrieval='rag_bm25',\n",
    "                    icl=i,\n",
    "                    name=f'Rag BM25@{i} [70B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/llamav2/70b/full_sample_{task}Exam_rag_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:70B',\n",
    "                     retrieval='rag_multi_qa',\n",
    "                     icl=i,\n",
    "                     name=f'Rag MultiQA@{i} [70B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/llamav2/70b/full_sample_{task}Exam_rag_dpr_bm25_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='llamav2:70B',\n",
    "                     retrieval='rag_dprv2',\n",
    "                     icl=i,\n",
    "                     name=f'Rag DPRV2@{i} [70B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}Exam/llamav2/70b/full_sample_{task}Exam_open_book_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='llamav2:70B',\n",
    "                    retrieval='open_book',\n",
    "                    icl=i,\n",
    "                    name=f'Oracle@{i} [70B]')] for i in range(3)],\n",
    "    )\n",
    "\n",
    "    # Add Mistral:7B Models\n",
    "    extended_students.extend([[\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}Exam/mistral/7b/full_sample_{task}Exam_closed_book_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='mistral:7b',\n",
    "                    retrieval='closed_book',\n",
    "                    icl=i,\n",
    "                    name=f'ClosedBook@{i} [7B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/mistral/7b/full_sample_{task}Exam_rag_siamese_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='mistral:7b',\n",
    "                    retrieval='rag_siamese',\n",
    "                    icl=i,\n",
    "                    name=f'Rag Siamese@{i} [7B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/mistral/7b/full_sample_{task}Exam_rag_dpr_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='mistral:7b',\n",
    "                    retrieval='rag_dpr',\n",
    "                    icl=i,\n",
    "                    name=f'Rag DPR@{i} [7B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}RagExam/mistral/7b/full_sample_{task}Exam_rag_bm25_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='mistral:7b',\n",
    "                    retrieval='rag_bm25',\n",
    "                    icl=i,\n",
    "                    name=f'Rag BM25@{i} [7B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/mistral/7b/full_sample_{task}Exam_rag_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='mistral:7b',\n",
    "                     retrieval='rag_multi_qa',\n",
    "                     icl=i,\n",
    "                     name=f'Rag MultiQA@{i} [7B]'),\n",
    "         ExamSetting(path_pattern=f'{root_path}/{task}NewRagExam/mistral/7b/full_sample_{task}Exam_rag_dpr_bm25_multi_qa_{model}_results_*_icl{i}.jsonl',\n",
    "                     llm='mistral:7b',\n",
    "                     retrieval='rag_dprv2',\n",
    "                     icl=i,\n",
    "                     name=f'Rag DPRV2@{i} [7B]'),\n",
    "        ExamSetting(path_pattern=f'{root_path}/{task}Exam/mistral/7b/full_sample_{task}Exam_open_book_{model}_results_*_icl{i}.jsonl',\n",
    "                    llm='mistral:7b',\n",
    "                    retrieval='open_book',\n",
    "                    icl=i,\n",
    "                    name=f'Oracle@{i} [7B]')] for i in range(3)],\n",
    "    )\n",
    "\n",
    "    return [i for elem in extended_students for i in elem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_file_in_folder(folder_path):\n",
    "    # List all entries in the given folder\n",
    "    entries = os.listdir(folder_path)\n",
    "\n",
    "    # Filter out only the files (excluding directories and other types)\n",
    "    files = [os.path.join(folder_path, f) for f in entries if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    # Check the number of files\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    elif len(files) == 0:\n",
    "        raise ValueError(f\"No files found in the directory {folder_path}\")\n",
    "    else:\n",
    "        raise ValueError(f\"More than one file found in the directory {folder_path}. Files are: {', '.join(files)}\")\n",
    "\n",
    "def get_documentation_data(task: str) -> Dict[str, str]:\n",
    "\n",
    "    main_folder = 'main_v1' if task == 'StackExchange' else 'main'\n",
    "    root_path = f'{Path(\".\").resolve().parent}/Data/{task}/KnowledgeCorpus/{main_folder}'\n",
    "\n",
    "    documentation_path = get_single_file_in_folder(root_path)\n",
    "    with open(documentation_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_from_html(text: str):\n",
    "\n",
    "    return text.split('/')[4].capitalize() if 'html' not in text.split('/')[4] else text.split('/')[3].capitalize()\n",
    "\n",
    "\n",
    "def get_topic_from_stack(source_list: List[str]) -> str:\n",
    "\n",
    "        filtered_list = list(set([elem.replace('https://', '').split('/')[0].split('.')[0] for elem in source_list]))\n",
    "\n",
    "        return filtered_list[0]\n",
    "\n",
    "def get_documentation_topic_list(qna_doc: Dict[str, str], ref_data: Dict[str, str], task: str) -> str:\n",
    "\n",
    "    try:\n",
    "\n",
    "        matching_documentation_list = [elem for elem in ref_data if ref_data if elem['text']==qna_doc['doc']['documentation']]\n",
    "\n",
    "        if len(matching_documentation_list) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            matching_documentation = matching_documentation_list[0]\n",
    "\n",
    "        if task == 'DevOps':\n",
    "\n",
    "            return matching_documentation['section']\n",
    "        \n",
    "        elif task == 'StackExchange':\n",
    "\n",
    "            return [get_topic_from_stack(matching_documentation['source'])]\n",
    "        \n",
    "        elif task == 'Arxiv':\n",
    "\n",
    "            return [i.split('.')[0] for i in matching_documentation['section']]\n",
    "        \n",
    "        elif task == 'SecFilings':\n",
    "\n",
    "            sec_cat = [\"ACME UNITED CORP\",\n",
    "                        \"ABBOTT LABORATORIES\",\n",
    "                        \"AMD (ADVANCED MICRO DEVICES)\",\n",
    "                        \"BK Technologies Corporation\",\n",
    "                        \"AAR CORP\",\n",
    "                        \"Air Products and Chemicals\",\n",
    "                        \"CECO Environmental Corp\",\n",
    "                        \"Worlds Online\",\n",
    "                        \"ADAMS RESOURCES & ENERGY\",\n",
    "                        \"Matson\"]\n",
    "\n",
    "            return [comp for comp in sec_cat if comp in qna_doc['doc']['documentation'].split(':')[0]]\n",
    "\n",
    "        \n",
    "    except:\n",
    "\n",
    "        print('Failure')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_documentation_topic(data[0]['doc']['documentation'], documentation)\n",
    "\n",
    "def get_acc(exam_setting: ExamSetting, \n",
    "            task: str, \n",
    "            categories: List[str]):\n",
    "\n",
    "    documentation = get_documentation_data(task)\n",
    "\n",
    "    with open(exam_setting.data_path, 'r') as f:\n",
    "        exam_doc = [json.loads(line) for line in f]\n",
    "\n",
    "    acc = {k: {'n_pts': 0, 'correct_classif': 0, 'acc': 0} for k in categories}\n",
    "\n",
    "    for qna_doc in exam_doc:\n",
    "\n",
    "        topic_list = get_documentation_topic_list(qna_doc, documentation, task)\n",
    "\n",
    "        for topic in topic_list: \n",
    "\n",
    "            if topic in acc.keys():\n",
    "                acc[topic]['correct_classif'] += qna_doc['acc']\n",
    "                acc[topic]['n_pts'] += 1\n",
    "            else:\n",
    "                print(topic)\n",
    "\n",
    "\n",
    "    for topic in acc.keys():\n",
    "\n",
    "        acc[topic]['acc'] = 100*acc[topic]['correct_classif']/acc[topic]['n_pts'] if acc[topic]['n_pts'] else 0\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_acc_dict(task, cat_list):\n",
    "\n",
    "    return {exam.name: get_acc(exam, task, cat_list) \n",
    "               for exam in get_all_students('llamav2', task)\n",
    "               if exam.exists}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devops_cat = ['Beanstalk', 'SageMaker', 'Lambda', 'RDS', 'S3', 'Gateway', 'RedShift', 'CloudFront', 'EC2', 'ECS', 'Load Balancer', 'ALB', 'DynamoDB', 'SQS', 'ElastiCache', 'ELB']\n",
    "devops_acc_list = get_acc_dict('DevOps', devops_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_PTS = 10\n",
    "target_models = [\"ClosedBook@1 [13B]\", \n",
    "                 \"Rag MultiQA@1 [13B]\",\n",
    "                 \"Rag DPRV2@1 [13B]\" ,\n",
    "                 \"Oracle@1 [13B]\",\n",
    "                 #\"ClosedBook@0 [70B]\", \n",
    "                 #\"Rag MultiQA@0 [70B]\",\n",
    "                 #\"Rag DPRV2@0 [70B]\" ,\n",
    "                 #\"Oracle@0 [70B]\",\n",
    "                 \"ClosedBook@1 [70B]\", \n",
    "                 \"Rag MultiQA@1 [70B]\",\n",
    "                 \"Rag DPRV2@1 [70B]\" ,\n",
    "                 \"Oracle@1 [70B]\",\n",
    "                 \"ClosedBook@1 [7B]\", \n",
    "                 \"Rag MultiQA@1 [7B]\",\n",
    "                 \"Rag DPRV2@1 [7B]\" ,\n",
    "                 \"Oracle@1 [7B]\"\n",
    "                 ]\n",
    "\n",
    "acc_list_to_use = devops_acc_list\n",
    "\n",
    "\n",
    "scores_all = []\n",
    "\n",
    "for key, val in acc_list_to_use.items():\n",
    "\n",
    "    for cat, dict_val in acc_list_to_use[key].items():\n",
    "\n",
    "        scores_all.append({\"model\": key, \n",
    "                           \"Size\": key.split()[2] if len(key.split())==3 else key.split()[1],\n",
    "                           \"Retriever\": key.split()[1].split('@')[0] if len(key.split())==3 else key.split()[0].split('@')[0],\n",
    "                           \"category\": cat, \n",
    "                           \"score\": dict_val['acc']})\n",
    "\n",
    "filtered_cat = [k for k,v in acc_list_to_use['ClosedBook@0 [13B]'].items() if v['n_pts'] >=MIN_N_PTS]\n",
    "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
    "\n",
    "# sort by target_models\n",
    "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
    "\n",
    "df_score = pd.DataFrame(scores_target)\n",
    "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
    "df_score = df_score[df_score[\"category\"].isin(filtered_cat)]\n",
    "\n",
    "fig = px.line_polar(df_score, \n",
    "                    r = 'score', \n",
    "                    theta = 'category', \n",
    "                    line_close = True, \n",
    "                    category_orders = {\"category\": filtered_cat},\n",
    "                    color = 'Retriever', \n",
    "                    #markers=True, \n",
    "                    line_dash = 'Size',\n",
    "                    #symbol = 'retriever',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "# Setting the width and height\n",
    "fig.update_layout(width=1100, \n",
    "                  height=800,\n",
    "                  title = 'AWS DevOps Exam Evaluation',\n",
    "                  #title_font_size=27,\n",
    "                  font = {'size': 27,\n",
    "                          'family': 'Times',\n",
    "                          'color': \"black\"},\n",
    "                #   polar=dict(\n",
    "                #         radialaxis=dict(\n",
    "                #             #title='Radial Axis Title',\n",
    "                #             title_font_size=18,\n",
    "                #             tickfont_size=16,\n",
    "                #         ),\n",
    "                #         angularaxis=dict(\n",
    "                #             title_font_size=18,\n",
    "                #             tickfont_size=16,\n",
    "                #         )\n",
    "                #     ),\n",
    "                #     legend=dict(\n",
    "                #         font=dict(\n",
    "                #             size=16\n",
    "                #         )\n",
    "                #    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_cat = ['physics',\n",
    " 'astro-ph',\n",
    " 'cond-mat',\n",
    " 'econ',\n",
    " 'cs',\n",
    " 'nlin',\n",
    " 'q-fin',\n",
    " 'hep-th',\n",
    " 'gr-qc',\n",
    " 'hep-ph',\n",
    " 'stat',\n",
    " 'quant-ph',\n",
    " 'math',\n",
    " 'q-bio',\n",
    " 'eess',\n",
    " 'nucl-ex',\n",
    " 'hep-ex',\n",
    " 'nucl-th',\n",
    " 'hep-lat',\n",
    " 'math-ph']\n",
    "\n",
    "arxiv_acc_list = get_acc_dict('Arxiv', arxiv_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_PTS = 10\n",
    "target_models = [\"ClosedBook@1 [13B]\", \n",
    "                 \"Rag MultiQA@1 [13B]\",\n",
    "                 \"Rag DPRV2@1 [13B]\" ,\n",
    "                 \"Oracle@1 [13B]\",\n",
    "                 \"ClosedBook@0 [70B]\", \n",
    "                 \"Rag MultiQA@0 [70B]\",\n",
    "                 \"Rag DPRV2@0 [70B]\" ,\n",
    "                 \"Oracle@0 [70B]\",\n",
    "                 \"ClosedBook@1 [7B]\", \n",
    "                 \"Rag MultiQA@1 [7B]\",\n",
    "                 \"Rag DPRV2@1 [7B]\" ,\n",
    "                 \"Oracle@1 [7B]\"\n",
    "                 ]\n",
    "\n",
    "acc_list_to_use = arxiv_acc_list\n",
    "\n",
    "\n",
    "scores_all = []\n",
    "\n",
    "for key, val in acc_list_to_use.items():\n",
    "\n",
    "    for cat, dict_val in acc_list_to_use[key].items():\n",
    "\n",
    "        scores_all.append({\"model\": key, \n",
    "                           \"Size\": key.split()[2] if len(key.split())==3 else key.split()[1],\n",
    "                           \"Retriever\": key.split()[1].split('@')[0] if len(key.split())==3 else key.split()[0].split('@')[0],\n",
    "                           \"category\": cat, \n",
    "                           \"score\": dict_val['acc']})\n",
    "\n",
    "filtered_cat = [k for k,v in acc_list_to_use['ClosedBook@0 [13B]'].items() if v['n_pts'] >=MIN_N_PTS]\n",
    "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
    "\n",
    "# sort by target_models\n",
    "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
    "\n",
    "df_score = pd.DataFrame(scores_target)\n",
    "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
    "df_score = df_score[df_score[\"category\"].isin(filtered_cat)]\n",
    "\n",
    "fig = px.line_polar(df_score, \n",
    "                    r = 'score', \n",
    "                    theta = 'category', \n",
    "                    line_close = True, \n",
    "                    category_orders = {\"category\": filtered_cat},\n",
    "                    color = 'Retriever', \n",
    "                    #markers=True, \n",
    "                    line_dash = 'Size',\n",
    "                    #symbol = 'retriever',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "# Setting the width and height\n",
    "fig.update_layout(width=800,\n",
    "                  height=600,\n",
    "                  title = 'Arxiv Exam Evaluation',\n",
    "                  font = {'size': 18,\n",
    "                          'family': 'Times',\n",
    "                          'color': \"black\"})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_cat = [\"ACME UNITED CORP\",\n",
    "\"AAR CORP\",\n",
    "\"Matson\",\n",
    "\"BK Technologies Corporation\",\n",
    "\"ABBOTT LABORATORIES\",\n",
    "\"AAR CORP\",\n",
    "\"Air Products and Chemicals\",\n",
    "\"CECO Environmental Corp\",\n",
    "\"Worlds Online\",\n",
    "\"ADAMS RESOURCES & ENERGY\",\n",
    "\"AMD (ADVANCED MICRO DEVICES)\"]\n",
    "\n",
    "sec_acc_list = get_acc_dict('SecFilings', sec_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_PTS = 10\n",
    "target_models = [\"ClosedBook@1 [13B]\", \n",
    "                 \"Rag MultiQA@1 [13B]\",\n",
    "                 \"Rag DPRV2@1 [13B]\" ,\n",
    "                 \"Oracle@1 [13B]\",\n",
    "                 \"ClosedBook@1 [70B]\", \n",
    "                 \"Rag MultiQA@1 [70B]\",\n",
    "                 \"Rag DPRV2@1 [70B]\" ,\n",
    "                 \"Oracle@1 [70B]\",\n",
    "                 \"ClosedBook@1 [7B]\", \n",
    "                 \"Rag MultiQA@1 [7B]\",\n",
    "                 \"Rag DPRV2@1 [7B]\" ,\n",
    "                 \"Oracle@1 [7B]\"\n",
    "                 ]\n",
    "\n",
    "acc_list_to_use = sec_acc_list\n",
    "\n",
    "\n",
    "scores_all = []\n",
    "\n",
    "for key, val in acc_list_to_use.items():\n",
    "\n",
    "    for cat, dict_val in acc_list_to_use[key].items():\n",
    "\n",
    "        scores_all.append({\"model\": key, \n",
    "                           \"Size\": key.split()[2] if len(key.split())==3 else key.split()[1],\n",
    "                           \"Retriever\": key.split()[1].split('@')[0] if len(key.split())==3 else key.split()[0].split('@')[0],\n",
    "                           \"category\": cat, \n",
    "                           \"score\": dict_val['acc']})\n",
    "\n",
    "filtered_cat = [k for k,v in acc_list_to_use['ClosedBook@0 [13B]'].items() if v['n_pts'] >=MIN_N_PTS]\n",
    "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
    "\n",
    "# sort by target_models\n",
    "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
    "\n",
    "df_score = pd.DataFrame(scores_target)\n",
    "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
    "df_score = df_score[df_score[\"category\"].isin(filtered_cat)]\n",
    "\n",
    "fig = px.line_polar(df_score, \n",
    "                    r = 'score', \n",
    "                    theta = 'category', \n",
    "                    line_close = True, \n",
    "                    category_orders = {\"category\": filtered_cat},\n",
    "                    color = 'Retriever', \n",
    "                    #markers=True, \n",
    "                    line_dash = 'Size',\n",
    "                    #symbol = 'retriever',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "# Setting the width and height\n",
    "fig.update_layout(width=800,\n",
    "                  height=600,\n",
    "                  title = 'Sec Filings Exam Evaluation',\n",
    "                  #font = {#'size': 12,\n",
    "                          #'family': 'Times',\n",
    "                          #'color': \"black\"\n",
    "                          #}\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackExchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_topics = ['Stackoverflow', 'math', 'superuser',\n",
    "                'serverfault', 'askubuntu', 'electronics',\n",
    "                'physics', 'unix', 'tex', 'english',\n",
    "                'meta', 'apple', 'ell', 'gaming',\n",
    "                'stats', 'softwareengineering',\n",
    "                'mathoverflow', 'gis', 'diy', 'magento',\n",
    "                'salesforce',\n",
    "'eosio',\n",
    "'sharepoint',\n",
    "'raspberrypi',\n",
    "'salesforce',\n",
    "'wordpress',\n",
    "'history',\n",
    "'ux',\n",
    "'emacs',\n",
    "'ai']\n",
    "\n",
    "stack_acc_list = get_acc_dict('StackExchange', stack_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_PTS = 10\n",
    "target_models = [\"ClosedBook@1 [13B]\", \n",
    "                 \"Rag MultiQA@1 [13B]\",\n",
    "                 \"Rag DPRV2@1 [13B]\" ,\n",
    "                 \"Oracle@1 [13B]\",\n",
    "                 \"ClosedBook@1 [70B]\", \n",
    "                 \"Rag MultiQA@1 [70B]\",\n",
    "                 \"Rag DPRV2@1 [70B]\" ,\n",
    "                 \"Oracle@1 [70B]\",\n",
    "                 \"ClosedBook@1 [7B]\", \n",
    "                 \"Rag MultiQA@1 [7B]\",\n",
    "                 \"Rag DPRV2@1 [7B]\" ,\n",
    "                 \"Oracle@1 [7B]\"\n",
    "                 ]\n",
    "\n",
    "acc_list_to_use = stack_acc_list\n",
    "\n",
    "\n",
    "scores_all = []\n",
    "\n",
    "for key, val in acc_list_to_use.items():\n",
    "\n",
    "    for cat, dict_val in acc_list_to_use[key].items():\n",
    "\n",
    "        scores_all.append({\"model\": key, \n",
    "                           \"Size\": key.split()[2] if len(key.split())==3 else key.split()[1],\n",
    "                           \"Retriever\": key.split()[1].split('@')[0] if len(key.split())==3 else key.split()[0].split('@')[0],\n",
    "                           \"category\": cat, \n",
    "                           \"score\": dict_val['acc']})\n",
    "\n",
    "filtered_cat = [k for k,v in acc_list_to_use['ClosedBook@0 [13B]'].items() if v['n_pts'] >=MIN_N_PTS]\n",
    "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
    "\n",
    "# sort by target_models\n",
    "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
    "\n",
    "df_score = pd.DataFrame(scores_target)\n",
    "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
    "df_score = df_score[df_score[\"category\"].isin(filtered_cat)]\n",
    "\n",
    "fig = px.line_polar(df_score, \n",
    "                    r = 'score', \n",
    "                    theta = 'category', \n",
    "                    line_close = True, \n",
    "                    category_orders = {\"category\": filtered_cat},\n",
    "                    color = 'Retriever', \n",
    "                    #markers=True, \n",
    "                    line_dash = 'Size',\n",
    "                    #symbol = 'retriever',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "# Setting the width and height\n",
    "fig.update_layout(width=800, height=600, title = 'Sec Filings Exam Evaluation')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
